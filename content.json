{"meta":{"title":"Neuroblog","subtitle":"","description":"","author":"AgeYY","url":"https://ageyy.github.io/neuroblog","root":"/neuroblog/"},"pages":[{"title":"Categories","date":"2020-09-09T09:41:47.422Z","updated":"2020-09-09T09:41:47.422Z","comments":true,"path":"categoties/index.html","permalink":"https://ageyy.github.io/neuroblog/categoties/index.html","excerpt":"","text":""}],"posts":[{"title":"Theory of Grid Cell","slug":"gridcell","date":"2020-09-28T04:10:38.000Z","updated":"2020-10-22T04:08:52.489Z","comments":true,"path":"2020/09/28/gridcell/","link":"","permalink":"https://ageyy.github.io/neuroblog/2020/09/28/gridcell/","excerpt":"An brief overview on my talk, Theory of Grid cell. Please refer to my powerpoint for figures and more detail.","text":"An brief overview on my talk, Theory of Grid cell. Please refer to my powerpoint for figures and more detail. Overview of the TalkNavigation is one of the fundamental cognitive ability exist in a wide range of animals. Yet its mechanism remains unclear. There are several types of cells are related to this ability. For example, Place cell has a receptive field to only one particular location (video). Grid cell has fasnating hexagonal receptive field (Nancy’s talk, more biological properties please refer the original paper [Torkel Hafting et al., 2005]). The biological meaning of the place cell is obvious, yet how the hexagonal pattern assist the agent to navigate remains unclear. Mechanistic Model succeeded in reproducing the hexagonal pattern though manually constructing the wiring of the grid cell networks.[Y Burak &amp; I Fiete, 2009][M Fuhs et al, 2006] Yet it didn’t convincely exlain why should the cells has such architechture. Nomative model, on the other hand, have much clearer picture by connecting the grid cell with the place cell or/and head cell etc. 1-layer model used the place cell information as input, few numbers as output, and Hebb rule as learning algorithm, plus a non-negative constrain, to do an unsupervised learning. The weight between input and output layer superisingly has the hexagonal pattern. [Yedidyah Dordek et al.]. This network can be interpreted as an encoder, which encodes the place cell information to few numbers. Therefore grid cell will naturally occur if you want to conpress the place cell information. The second nomative model is based on the RNN[C Cueva &amp; X wei, 2018]. The velocity (speed and direction) of the agent is the input, the output is the place cell information. RNN is used as the hidden layer to convert velocity into location. After traning with sufficient data, the hidden unit apears to have hexagonal receptive field. Furthermore, other type of cells, such as border cell and band cell are also observed. Therefore, grid cell in here is for decoding velocity into location. The above models can be unified with one theory [Ben Sorscher et al., 2019]. The common sub-problem in the two nomative model is how can you related the grid cell to produce the place cell information. Therefore the following loss function is constructed\\begin{equation}L = | P - \\hat{P} |,\\label{loss}\\end{equation} where $P$ is the place cell information, $\\hat{P}$ is the predicted place cell information based on the grid cell. With some simplification, the author analytically showed the grid pattern will occur while minimizing this loss function, so the two nomative models are unified. While the dynamic equation in the mechanistic model is only the gradient descent equation of the Lagrangian of eqaution (\\ref{loss}). Disscusion[Ben Sorscher et al., 2019] gave very good conclusion on the grid cell problem. Here I only mention my one thought. The above theory not only partially solved the grid cell problem, but also showed a very clear logic of how can we understanding our brain. The first step is to construct a proper artificial network which can reproduce at least some properties of the sensory cortex. This should be done by creativly setting the architechture, learning rule and objective function. This re-representation could offer clear explaination of the original biological cortex (for example, see visual cortex in my another post). Then, a theory should be applied in explaining the artificial neural network, which can offer more insight on how our brain works.","categories":[{"name":"Ref Reading","slug":"Ref-Reading","permalink":"https://ageyy.github.io/neuroblog/categories/Ref-Reading/"}],"tags":[]},{"title":"Understanding Sensory Cortex via Hierachical Convolutional Neural Networks.","slug":"hcnn","date":"2020-09-16T13:49:48.000Z","updated":"2020-10-18T17:50:40.081Z","comments":true,"path":"2020/09/16/hcnn/","link":"","permalink":"https://ageyy.github.io/neuroblog/2020/09/16/hcnn/","excerpt":"The skeleton of this short review is mainly based on Using goal-driven deep learning models to understanding sensory cortex","text":"The skeleton of this short review is mainly based on Using goal-driven deep learning models to understanding sensory cortex IntroductionOne ultimate goal of the Neuroscience is to predict the reponse of even a single neuron, given a particular stimulus (i.e. how the neuron encode the information). In terms of this goal, visual cortex is probably the most well-understood cortex. Therefore this short review will start from brief biological introduction to the central visual pathway. Then move to how the traditional methods taken efforts in making predition of the response. However, these traditional method failed to predict the response of neurons in higher order of cortex region, such as in V4 or inferior temporal (IT) cortex. Therefore, in section 3, we introduce a generative model: Hierachical Convolutional Neural Networks (HCNN), equip it with goal-driven philosiphy, to obtain pretty good predition even with higher order of cortex region. The final section includes some potentials and limitations of this HCNN $+$ goal-driven method. Central Visual PathwayCentral visual pathway means how the information of the images (represented by photons) is converted into the electric signal, and be transmitted in the cortex. The photons firstly hit the retina in our eyes, change the conductance of the channels in our photo-receptors, thus change strength the electuric signals. This signals is transmitted by ganglion cells along the optic nerve to reach the Lateral geniculate nucleus (LGN) in the thalamus (FIG. 1). Optic radiation radiate the signals in LGN to the V1 cortex in the most posterior of the occipital lobe. Now there are basicaly two pathway for the signal to travel. (FIG.) [S. Williams &amp; L. White, 2013] Dorsal pathway: V1, V2, V3, V3A and MT etc. This pathway is thought of responsible for the “where” problem: what is the positional relation of the objects in the image, in which direction does the object move… Ventral pathway: V1, V2, V4 and IT etc. This pathway is thought of responsible for the “what” problem, such as object recogization. One goal of today’s neuroscience is to understanding how the neurons in the above regions response to different stimulus (i.e. images). More formally, the stimulus is a matrix $S$, with each entry represents the photon density, the questions finding the firing rate of the neurons in a very small regions (so they can be thought of react to the stimulus homogenusly) $r$ as a function of matrix $S$. Traditional MethodGabor wavelet filterThe most straight forward model of the above problem is receptive field\\begin{equation}r = \\sum_{i,j} R_{i,j}S_{i,j}\\end{equation} where $R$ is the receptive field. It can be either a matrix (descret positions) or function R(x, y) (continum positions). According to some observation, JP Jones etal in year 1987 propsed that R is actually a gabor wavelet [J Jones &amp; L Palmer]\\begin{equation}R(x, y) = K \\text{exp}(-1/2(x_g^2 / a^2 + y_g^2 / b^2)) \\text{cos}(-2\\pi(U_0x + V_0y) - P),\\label{gaborwave}\\end{equation} where $x_g$ and $y_g$ are the rotation transfomation of the original coordinate space\\begin{equation}x_g = x \\text{cos}(\\theta) - y \\text{sin}(\\theta);y_g = x \\text{sin}(\\theta) + y \\text{cos}(\\theta)\\end{equation} The rest of parameters in equation (\\ref{gaborwave}) are constant. The fitting result shows pretty good result of the neurons in the V1 cortex (FIG.3) However, this method only tells what the receptive field should look like, but didn’t clearly expain why they should be gabor wavelet. Learning the Neural Response via Efficient CodingOn the contrary, efficient coding could give a clearer biological meaning. An interpretation of the function of neurons is that, different neurons are developed to capture different aspects of the images. For example, neuron 1 will only detect the image information in the horizonal direction, neuron 2 will only look at into the infomation in another direction. So neurons could actually tears the original image into some simple parts which is easier for further analysing. Actually this idea could be achieved by gabor function: the response via different gabor wavelet filters can indeed capture different aspects of the original image. Yet efficient coding is a more powerful method because it could learn the response automatically. With the above idea, the image can be represented by a set of basis [BOlshausen &amp; D. Field, 1996]\\begin{equation}S(x, y) = \\sum_{i} a_i \\phi_i(x, y).\\end{equation} where index $i$ represents neuron $i$. Our brain must evolved at some point between the ability (how well we can understand the image) and economy (it should take as fewer neurons as possible). This two aspects hint us to write down the following loss function\\begin{equation}L = - [\\text{perserve information}] - \\lambda [\\text{sparseness of $a_i$}]\\label{lossinfo}\\end{equation} where $\\lambda$ controls the importance of economy. Perserve information could be the L2 error\\begin{equation}[\\text{perserve information}] = - [S(x, y) - \\sum_{i} a_i \\phi_i(x, y)]^2\\end{equation} and\\begin{equation}[\\text{sparseness of $a_i$}] = - \\sum_i S(\\frac{a_i}{\\sigma})\\end{equation} Equation (\\ref{lossinfo}) can be minimized though gradient descent. We could use nature scence as the traning set, and do the unsupervised learning so could obtain the basis (FIG. 4). It correctly shows how different neurons capture different aspects of the original image. Fitting Networks to Neural DataThis method is even more straightforward. We could obtain the stimulus and the response of the neurons from the experiment, therefore we could link this two quantities directlly though some statistics method. This method apears to be succesful in visual area V1, auditory area A1 etc. [R deCharms and A Zador, 2000] Difficulties with Deeper NetworksThe above models are successful in the primary cortex (V1). However, the higher order region like V4 apears to be too complex so that either asumptions in the above method become invalid nor too difficult to fit due to the too many parameters. Therefore new method is needed. HCNN and Goal-driven MethodAlthough we don’t exactly know how neurons in the cortex wiring together, we could roughly divide cortex into layers (under the meaning of connections, not anatomy relation). For example, V1, V2 are different layers. There are infomation flows between layers to layers. One good anology of the cotex is Hierachical Convolutional Neural Networks (FIG. 5). Each entry in one layers is a number, corresponds to the firing rate of one small region of neurons. Layers are connected though linear-non-linear (LN) operation: filter, threshold, pool and normalize. Each sub-operation can roughly corresponds to some features of the cortex. For example, threshold is the activation function of one neuron under a stimulus. HCNN contains majorly two kinds of parameters: 1. discrete paramters, includ the number of layers, shape of the filters etc. 2. continuous paramters, the weights on the filter and fully connected layers. From the traditional method, we know it is difficult to findout the stimulus-response relation. Therefore in goal-driven method, we temperally don’t consider this problem. Instead, we consider how to use HCNN to reproduce the cognitive ability, for example, in ventral visual patheway, how to recognize objects. The parameters above are configured by optimizing the performance of the cognitive abilitt. Surperising, if we detect the response of the neurons in hidden layer, it will give good prediction of how the real neuron response (FIG. 5). The most powerful aspect of HCNN I think is that, it can unify different sensory cortex into one architechture. HCNN is not only applicable to visual cortex, but with some modification, also applicable to auditory cortex. This implies that our brain was actually a giant, simple HCNN. The evolution force us to “learn” the exact parameters to achieve different congnative ability. Actually, if part of funtion in our brain is injured, we could possible use the other part of the brain to perform the same task, which means the cortex specialied in one function can smoothly transformed into another specialization. [S. Carron ect.] DisscusionMost of the potential and limitations are discussed in [D Yamins ectal., 2018]. Here I would only add some points. The most limitation of the HCNN I suppose is the architechture. Convolution apears only for some invariant properties in the image. It is likely that “filter” neurons developed same parameters, so that we could use single filter to replace them. However, there’s no reason that the other sensory cotex have some similar symmetry. Further, recurrent seems exist everywhere in the cortex, yet this important feature is not inclued in the HCNN model. Therefore better understanding on how to coorperate RNN in the brain to the machine learning algorithm is needed. [G Yang &amp; X Wang, 2020] Further, even we could reproduce the response of neuron via HCNN model, we still don’t know why they response like that. HCNN model, unfortrunately lack of good interpretability. This is related to the long existing reverse engineering problem. Image Ref[1] S. Williams &amp; L. White, 2013[2] J Jones &amp; L Palmer[3] BOlshausen &amp; D. Field, 1996[4] BOlshausen &amp; D. Field, 1996","categories":[{"name":"Ref Reading","slug":"Ref-Reading","permalink":"https://ageyy.github.io/neuroblog/categories/Ref-Reading/"}],"tags":[{"name":"Visual System","slug":"Visual-System","permalink":"https://ageyy.github.io/neuroblog/tags/Visual-System/"},{"name":"HCNN","slug":"HCNN","permalink":"https://ageyy.github.io/neuroblog/tags/HCNN/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://ageyy.github.io/neuroblog/tags/Machine-Learning/"}]},{"title":"Modeling of Simple Neural Network","slug":"biophysics","date":"2020-09-09T09:15:48.000Z","updated":"2020-10-18T17:50:48.745Z","comments":true,"path":"2020/09/09/biophysics/","link":"","permalink":"https://ageyy.github.io/neuroblog/2020/09/09/biophysics/","excerpt":"Mathematical modeling from sub-cell components of a neuron to simple neural network. As part of course Introduction to Neural Computation","text":"Mathematical modeling from sub-cell components of a neuron to simple neural network. As part of course Introduction to Neural Computation IntroductionResearchers study our neural system in different levels: from components of a single cell, such as membrane, to the connection of two cells (synapsis), and further to the complex network of the whole brain. This summary trying to find a straight pathway of the modeling from sub-cellcular level to simple neural network. Please refer Introduction to Neural Computation for more detail. MembraneThe membrane has roughly two parts that are vital in the neural activity: phospholipid bilayer and channels (FIG. 1). Phospholipid bilayer is permeable to water, oil etc., yet impereable for ions. This attribution is similar to semi-permeable membrane. Ions carry charges, if the charge densities of the two sides of phospholipid bilayer (intra cellular space and extra-cellular space) are different, there will be a voltage difference across cell membrane. In all, because of the existence of phospholipid bilayer, the cell has a function similar to capacitor\\begin{equation}c V = Q\\label{ohm}\\end{equation} where $c$ is the capacity per unit area, $Q$ is the charge density differences and $V$ is the voltage accross the cell. The voltage of the extra-cellcular space is set to be zero, therefore $V$ here is also the voltage of intra-cellular space, which is roughly $-70 \\text{mV}$ to $ -50 \\text{mV}$ as measured by the experiment. There are two types of channels: ion channels and metabolic channel. The effect of metabolic channels are more indirect and complex, which is not the central concern of this summary. Ion channel behaviors as the gate of the membrane. It allows only certain types of ions to pass through the membrane, thus produce inward or outward current flow. For example, Na-gated channel is only permeable to Na$^+$, when the channel is open . One good approximation of the channel is resistor, govern by the Ohm’s law\\begin{equation}I = G_i (V - V_i) \\label{olm}\\end{equation} where $G_i$ is the conductance which can depend on the votlage and time, and $V_i$ is the equilibrium voltage (constant for a certain type of channel) results from the balance of electric force and diffusion (FIG. 2). Index $i$ here means $i$th type of ion channel. Positive $I$ means outward current. Single CellDifferentiate on both side of equation (\\ref{ohm}) and use equation (\\ref{olm}), we have an equation for single cell\\begin{equation}c \\frac{dV}{dt} = - G_L (V - V_L) - I_e\\label{ifmodel}\\end{equation} where $I_e$ is the extra-induced current, such as the experimentist inject ions to the cell (negtive $I_e$). Index $L$ is leakage channel, which is a mathematically equivalent channel of a partial function of some types of channel (largely fall into K-selective channel). The rest of effects of channels can be simplified as the following statement: If $V &gt; V_{th}$, $V = 50 \\text{mV}$, then quickly fall to $V_{res}$. This effect is often introduced by Na-selective channels. Typical value of $V_{res}$ is $-65 mV$. There are some choices for $V_{th}$, but values like $-30 mV$ is sufficient to fit a good result. When this statement is triggered, we say the neuron fired, or it made a spike. This single-cell model also be named Integrate and Firing Model. This model contains one resistor (leakage channel), one conductance (membrane), one external current source ($I_e$), and voltage source (FIG. 3). There are lots more sophisticated models, such as HH model, which replace the statement with more realistic channels as additional terms in equation (\\ref{ifmodel}). Equation (\\ref{ifmodel}) can be collected with a more compact form\\begin{equation}\\tau \\frac{dV}{dt} = -V + V_{\\infty}, \\label{ifmodelS} \\end{equation} where $\\tau = c / G_L$ and $V_{\\infty} = (V_L - I_e) / G_L$. If $G_L$ here is a constant, solution of equation [ifmodelS] is\\begin{equation}V = (V_L - V_{\\infty}) e^{-t/\\tau} + V_{\\infty}.\\end{equation} The firing pattern is shown in FIG. 4 Information is stored in the firing rate of neurons instead of the exact value of voltage. Firing rate is defined as the number of spikes in a unit time. The time spacing between two spikes can be calculated from equation (\\ref{ifmodelS}) by calculate the time that $V$ grows from $V_{res}$ to $V_{th}$. The inverse of time spacing is firing rate, as shown in FIG. 5. In all, we could conclude the relation with\\begin{equation}v = F[I_e]\\label{fireateeq}\\end{equation} where $v$ is the firing rate, and $F$ is some function has the shape in FIG. 5 SynapseBefore moving to neural network, we need to understand how two neurons connect each other by synapse. FIG. [syn] shows the biological structure of the synapse. When the pre-neuron is fired, the depolyzation will open the Ca-channel, induces an inward flow of Ca$^{2+}$. This Ca$^{2+}$ will release the transmitters in the presynapse. Transmitter in the synapse cleft then attach to particular receptors in the post-synapse, open ion-channel to allow Na$^+$ flow into the post-synapse. This final lead to the depolarization of the post-neuron. The current flow into the post-neuron can be described by\\begin{equation}I_{syn} = - G_{syn}(t) (V - V_{syn}), \\label{isyn}\\end{equation} where $V_{syn}$ is about $0$ mV, which is the reversal potential of the post-synapse. Conductance $G_{syn}(t)$ behaviors like a exponential decaying function. If there’s no transmitters in the synapse cleft, ion channels in the post-synapse are closed, $G_{syn}(t) = 0$ mV. A spike of the presynapse will suddenly release a large amount of transmitters, therefore largely increase $G_{syn}(t)$. Then the transmitters will graduatly either diffuse away or be decomposed by glials, which lead to the slow decay of the conductance. The conductance induced by single spike is often approximated by the exponential function\\begin{equation}K(t) = G_{max} e^{-t / \\tau}.\\end{equation} The whole conductance under a train of spikes is\\begin{equation}G(t) = \\sum_iK(t) \\delta_{t,t_i}\\label{spikeraw}\\end{equation} where there is a spike at time $t_i$. Equation (\\ref{spikeraw}) can be simplified with the spike train $S(t) = \\sum_i\\delta(t - t_i)$\\begin{equation}G(t) = \\int{d\\tau}K(\\tau)S(t - \\tau)\\end{equation} Simple Neural NetworkConsider a network with only two neurons attached. The working status of each neuron is solely stated by its firing rate. So we need to findout the firing-rate relation between neurons. For the single neuron, the only source of external current is from its pre-neuron, i.e. $v = F[I_{syn}]$ from equation (\\ref{fireateeq}). Yet $I_{syn}$ followed by equation (\\ref{isyn}) is still too complex to be pratical. Hence we further throw away the voltage term, and treat $G_{syn}(t)$ as voltage-independent,\\begin{equation}I_{syn} = \\int{d\\tau}K(\\tau)S(t - \\tau).\\end{equation} we could extract a factor $w$ so that the rest of $U(\\tau) = K(\\tau) / w$ can be normalized,\\begin{equation}\\int{d\\tau}U(\\tau) = 1\\end{equation} The convolution of a narrow (correnspond to short time of non-zero $K(\\tau)$) window $U(\\tau)$ with spike train $S(\\tau)$ is exactly the firing rate of the presynapse $u$. Overall, we have\\begin{equation}v = F[w u].\\end{equation} This equation reveals the simple relation of the network. Larger $w$ means the two neurons are more strongly connect, only a slow firing rate of the pre-neuron is enough for triggering the post-neuron. This biologically could mean there are more channels in the post-synapse. Different presynapse will attarch to different sites of the postsynapse. Furthermore, the post-neual layer can also attarch to each other as shown in FIG. 7. The overall effect is the summation of all their current.\\begin{equation}\\boldsymbol{v} = F[W\\boldsymbol{u} + M \\boldsymbol{v}],\\end{equation} where $W, M$ are matrices, arrays are written as bold letters. This model is called rate model, which is the fundation of the lots of deep learning architechture. ConclusionThe membrane of the neuron is a combination of resistor and capacity. Different conductance of different ion channels allow the neuron to fire, and soon go back to the resting potential. A spike from the pre-neuron will trigger a bit depolarization of the post-neuron. Once the potential is high enough, the post-neuron will also make spikes. Despite the complicated procedure for generating a spike, the exact voltage is unimportant. The real information is stored in the frequency of the spikes, which can be described using a single number – firing rate. By carefully simplifing the synapse, one could obtain the relation between the firing rates of different neurons, thus have an equation for the neuron network. This neuron network lies in the basis current deep network algorithms. Ref[1] Wiki: cellular membrane[2] Chapter 3 of Neuroscience: Exploring the Brain[3] Lecture 3 of MIT OpenCourseWare: Introduction to Neural Computation[4] Chapter 5 of Neuroscience: Exploring the Brain[5] Lecture 18 of MIT OpenCourseWare: Introduction to Neural Computation","categories":[{"name":"Ref Reading","slug":"Ref-Reading","permalink":"https://ageyy.github.io/neuroblog/categories/Ref-Reading/"}],"tags":[]}],"categories":[{"name":"Ref Reading","slug":"Ref-Reading","permalink":"https://ageyy.github.io/neuroblog/categories/Ref-Reading/"}],"tags":[{"name":"Visual System","slug":"Visual-System","permalink":"https://ageyy.github.io/neuroblog/tags/Visual-System/"},{"name":"HCNN","slug":"HCNN","permalink":"https://ageyy.github.io/neuroblog/tags/HCNN/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://ageyy.github.io/neuroblog/tags/Machine-Learning/"}]}