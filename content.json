{"meta":{"title":"Neuroblog","subtitle":"","description":"","author":"AgeYY","url":"https://ageyy.github.io/neuroblog","root":"/neuroblog/"},"pages":[{"title":"Categories","date":"2020-09-09T09:41:47.422Z","updated":"2020-09-09T09:41:47.422Z","comments":true,"path":"categoties/index.html","permalink":"https://ageyy.github.io/neuroblog/categoties/index.html","excerpt":"","text":""}],"posts":[{"title":"Dynamics for Working Memory and Time Encoding","slug":"dynWM","date":"2021-02-06T02:40:56.042Z","updated":"2021-02-06T04:11:26.521Z","comments":true,"path":"2021/02/06/dynWM/","link":"","permalink":"https://ageyy.github.io/neuroblog/2021/02/06/dynWM/","excerpt":"How information is encoded in working memory? See power point here.","text":"How information is encoded in working memory? See power point here. SummaryTo maximize the future rewards of the changing world, we must be able to discover the temporal structure of the stimulus and react accordingly at the right time. So what is the underlying mechanism of encoding information in working memory? To address this problem, [Christopher Cueva, et al., 2020] summarized three hypotheses: information is stored in attractors, low dimensional trajectory, and high dimensional chaotic trajectory, and tested them experimentally. They showed time can be decoded with high precision in task where timing information is relevant. The neural trajectory is always in low dimension. This implies that low dimensional representation is a promising mechanism for working memory. CommentI am pretty interested in the data analysis method of this paper. Within the attractor hyperthesis, the neural activity of two time points in the delay period should be quite similar. In the paper, they measure the similarity by classification. Firstly, in a single trial, the neural activity in each time point (with time-averaged small interval) is actually a dot in the high dimensional space. So the similarity of two time points can be measured directly by their distance. However, the experimental data contains noise, therefore they considered all trials as many dots in the high dimensional space. Type 1 dots are the representation of time interval 1, type 2 dots are the representation of time interval 2. A classifier can be used to classify these two categories. The classification accuracy then is the measure of the similarity: high accuracy means low similarity. Another method is the measuring of dimensionality. They split the data to train and test sets. Training data is projected into low dimensional space by PCA. The dimensionality is defined as the numbers of principal components when the error between the projected training data and test data are minimized. The philosophy of this method is that both training data and test are constructed from same kernel vectors plus independent noise. The first few principal components capture the kernel vector. By gradually increasing the number of PCs, the projected training data become similar to the test data. However, too many PCs will also capture the noise, which will also lead to error. So when the error is minimized, only the dimensionality of the true kernel will be captured, this is the dimensionality of the task. I would like to compare this method with another simple one proposed by [Peiran Gao, et al., 2017]. Dimensions is defined as the number of PCs which can explain a threshold of variance (such 70%). The advantage of this method is its simplicity. However, this method is based on human intuition on the threshold. It can either fail to capture the kernel vectors (too low threshold), or also include the dimension of noise (too high threshold). Finally, the paper’s result is also related to my recent result. I built an RNN to learn simple tasks. Its neural activity is plotted in low dimensional space by PCA, which showed a quite smooth trajectory. However, this does not mean that the working memory has no attractors. In the decision-making task, if I extend the delay time, finally the trajectory will converge to two attractors, despite the RNN is only trained in a short time. While in the encoding spatial information task, the long term behavior of RNN will diverge. It is indeed an interesting topic to explore how the neural circuit encoding discrete information and continuous information.","categories":[{"name":"Ref Reading","slug":"Ref-Reading","permalink":"https://ageyy.github.io/neuroblog/categories/Ref-Reading/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"https://ageyy.github.io/neuroblog/tags/RNN/"},{"name":"Dynamical System","slug":"Dynamical-System","permalink":"https://ageyy.github.io/neuroblog/tags/Dynamical-System/"},{"name":"Working Memory","slug":"Working-Memory","permalink":"https://ageyy.github.io/neuroblog/tags/Working-Memory/"}]},{"title":"Autistic Spectrum Disorder From Dynamical Perspective","slug":"asdynamic","date":"2021-01-09T07:08:18.996Z","updated":"2021-01-09T07:35:12.008Z","comments":true,"path":"2021/01/09/asdynamic/","link":"","permalink":"https://ageyy.github.io/neuroblog/2021/01/09/asdynamic/","excerpt":"Finding the causes of autistic spectrum disorder (ASD) from the perspective of dynamical system. Thumbnail adapted from [Shenoy et al. 2013]","text":"Finding the causes of autistic spectrum disorder (ASD) from the perspective of dynamical system. Thumbnail adapted from [Shenoy et al. 2013] AbstractContinous changing brain activity can be viewed as a trajectory in the high dimensional state space, with each dimension as a neuron. Mental illness is hyperthesis of the mulfunction of the individual’s state space. However, this view seems have not been applyed to the study of autism. Here, we proposed a bit detailed plan which is promising in finding the causes of autism from the dynamical point of view. BackgroundThe brain is believed as an interconnected network with billions of neurons as nodes.This giant network is possible to be described by a set of equations. Despite we don’texactly know the close form of the equation, it has already provided us a new picture – adynamic picture of viewing the neuron activities. In this picture, each state of the brainnetwork is a point in N-dimensional state space (N is the number of neurons). Adynamical changing of the neuron activity is equivalent to a trajectory in state space. This dynamical view helps us understand the brain. A common experimental paradigmused to study the motor cortex is the instructed-delay reaching task. From the dynamicalperspective, the preparatory period brings the neural state to an initial state, so thatneurons can quickly shift to the movement attractor after the ‘Go’ cue [1]. In decisionmaking, the researcher found the neural states in the state space form a line attractor: alinear arrangement of stable points with endpoints corresponding to converged decisions[2]. Chaisangmongkon et al. [3] trained RNN to perform match-to-category (DMC) tasks.They found that there are stable states connected by robust trajectory tunnels.Misclassification thus can be interpreted as noise pushing the state outside the correctchannel. Recently researchers also making efforts to extend this dynamical view to the wholebrain scale, especially in the study of mental illness. The central hypothesis is thatdramatic alternation of the attractor dynamics will cause mental illness. For instance,failure to integrate information is associated with alteration in line attractor [4].Obsession is caused by overly steep attractor basins. Too low chaoticity will lead toreduced cognitive flexibility. Indeed, based on the profound symptom, the Autism brainshould have some unusual features in the state space. Research PlanHere, we aim to uncover the possible abnormal features in the dynamics of Autism bythree stages. In the first stage, we plan to infer the underlying dynamics of the autismbrain from their fMRI data. The data will be acquired from Autism Brain Imaging DataExchange (ABIDE), which contains 1, 000 scanning subjects, with around 500 ASD and500 normal people. However, inferring the complete dynamics from such low temporalresolution data (TR = 2 seconds) could be a challenge. Therefore we would like to firsttry the traditional methods, such as Hidden Marte Calo Model, or Ising Model. Thesemodels although contain little information of the state space, they can find out simplefeatures, such as the deepness of the attractor, transitional probability between theattractor, etc. Indeed, it has been shown in [5] that ASD has abnormal states transition. In stage 2, we would like to infer more detail with Deep learning, specifically, RNN. Dueto the complexity of the brain data, some modification of the RNN is necessary.Fortunately, there are several modified architectures available recently from [6]. Thesemodels have been proven robust even when learning the underlying dynamics from thechaotic system, such as the Lorentz system. In the ideal world, each individual’s fMRIdata can be fed into one particular RNN, so that as the final outcome of this stage, wehave 1, 000 different RNNs as “fake brain”. These machines then are passed to stage 3for further analysis. There are lots of existing methods for analyzing RNN from both traditionalNeuroscience, and the recent active Machine Learning community. Traditional oneswould be finding the attractor, and study the linearized approximation behavior near theattractor. Mature packages (FixPointFinder [7]) have been developed for this kind ofstudy. We would like to go further, employ novel metrics such as Riemann curvature tomeasure the motion of the states in state space. Overall, the research is supposed to develop step-wise by: Using traditional models, such as HMM and Ising Model to study the simpledynamical features of the Autism group Training the RNN to learn the underlying dynamics of the individual brain fromfMRI data Analyzing the behavior of an individual’s RNN, obtain some novel dynamicfeatures. These features will be used to compare the ASD group and normalgroup, and even tell the individual difference within the ASD group. Some CommentsThis is actually an writing assignment. Honestly, after I finished the writing, I realized this plan is unlikely to work. The biggest challenge is still how RNN inffers the underlying dynamics from such sparse, low temporal-resolution data. Anyway, we could use EEG instead. Another problem is that, the brain dynamics could be far complex, which is unlikely to be learned by RNN. In the worst case, people doing this plan could struggle to design RNN architectures to do the mission of impossible. Ref[1] Shenoy, K. V., Sahani, M., &amp; Churchland, M. M. (2013). Cortical control of arm movements: Adynamical systems perspective. Annual Review of Neuroscience, 36, 337–359. [2] Mante, V., Sussillo, D., Shenoy, K. V., &amp; Newsome, W. T. (2013). Context-dependentcomputation by recurrent dynamics in prefrontal cortex. Nature, 503(7474), 78–84. [3] Chaisangmongkon, W., Swaminathan, S. K., Freedman, D. J., &amp; Wang, X. J. (2017). Computingby Robust Transience: How the Fronto-Parietal Network Performs Sequential, Category-BasedDecisions. Neuron, 93(6), 1504-1517.e4. [4] Durstewitz, D., Huys, Q. J. M., &amp; Koppe, G. (2020). Psychiatric Illnesses as Disorders ofNetwork Dynamics. Biological Psychiatry: Cognitive Neuroscience and Neuroimaging, Figure 1,1–12. [5] Watanabe, T., &amp; Rees, G. (2017). Brain network dynamics in high-functioning individuals withautism. Nature Communications, 8. [6] Koppe, G., Toutounji, H., Kirsch, P., Lis, S., &amp; Durstewitz, D. (2019). Identifying nonlineardynamical systems via generative recurrent neural networks with applications to fMRI. In PLoSComputational Biology (Vol. 15, Issue 8). [7] Golub and Sussillo (2018), “FixedPointFinder: A Tensorflow toolbox for identifying andcharacterizing fixed points in recurrent neural networks,” Journal of Open Source Software,3(31), 1003.","categories":[{"name":"Ideas","slug":"Ideas","permalink":"https://ageyy.github.io/neuroblog/categories/Ideas/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"https://ageyy.github.io/neuroblog/tags/RNN/"},{"name":"Dynamical System","slug":"Dynamical-System","permalink":"https://ageyy.github.io/neuroblog/tags/Dynamical-System/"},{"name":"Autistic Spectrum Disorder","slug":"Autistic-Spectrum-Disorder","permalink":"https://ageyy.github.io/neuroblog/tags/Autistic-Spectrum-Disorder/"}]},{"title":"RNN as a Tool for Neuroscience","slug":"rnnNeuro","date":"2021-01-02T08:05:09.249Z","updated":"2021-01-02T08:37:40.092Z","comments":true,"path":"2021/01/02/rnnNeuro/","link":"","permalink":"https://ageyy.github.io/neuroblog/2021/01/02/rnnNeuro/","excerpt":"Some random thoughts about using RNN as a tool in neuroscience research.","text":"Some random thoughts about using RNN as a tool in neuroscience research. Understanding the brain involves searching for a model mapping the neural activity to the behavior. This model could be qualitative. For instance, assigning each brain region with a behavior label. If a region shows a stronger firing rate compare with other regions, then the people is likely doing the corresponding label behavior. This is a basic idea of the theory of modularity. Although the recurrent neural network isn’t as successful as the theory of in the prediction of behavior, it is a quantitative model based on a realistic, microbiological neural network. This allows RNN to answer more fundamental questions, such as what is learning, why some neurons have such intriguing firing patterns, etc. One common RNN model is the rate model, which could be described by the following equation [Barak, O., Curr. Opin. Neurol., (2017)]:\\begin{equation}\\frac{dx_i}{dt} = -x_i + \\Sigma_j{J_{ij}\\phi(x_j)} + \\Sigma_k{B_{ik} u_k(t)}\\end{equation} Where $x_i$ is the input current of neuron $i$, $J$ is the structural connectivity, $\\phi(x_i)$ is the firing rate of neuron $i$ , matrix $B$ is the external input current to neural $i$. We have good approximation functions for $\\phi(x_i)$ , such as sigmoid function, ReLU, etc. Yet the structural connectivity and the external input remain largely undetectable. Indeed, before solving the equation, one needs to fit the structural connectivity and external input to the neural data. This could be done in two ways. (See FIG. 1 and [Barak, O., Curr. Opin. Neurol., (2017)]) The first one is the bottom-up approach. One could construct a proper form of the equation based on low dimensional intuition. Or more broadly, one good build other forms of models, such as the Ising model, to capture the underlying dynamic system of the brain. The advantage of this bottom-up approach is that they have very clear biological meanings and have only a few parameters. For example, structural connectivity in the whole brain level detected from DTI is directly used as the structural connectivity J in equation (1), with a prefactor to present the global coupling strength. However, this approach often fails to have good fitness on the realistic neural data. This is reasonable considering the model has only a few parameters but would like to fit the much more complex human brain. Another approach is the top-down approach. The behavior in equation (1) is often captured as a summary of the neural response ($\\phi(x)$), e.g. . A Loss function is defined to measure the distance of the simulated behavior z and the experimental behavior data. This approach is pretty successful in predicting the behavior, yet does the fitted J and B really have realistic biological meanings, or just artifacts only for fitting the behavior data? The answer to the above problem is directly comparing the simulated neural response ($\\phi(x)$) to the biological neural response. For instance, on the local level, the spiking train of each neuron is recorded. Then the PSTH is compared to artificial neural response ($\\phi(x)$) by SVCCA. Generally speaking, SVCCA selects a few principal components of two datasets (true neural response and $\\phi(x)$). Then it tried to align those components using linear transform, but in most cases, even it tried hard, there are still some differences between the two sets of components. This difference then is used to measure the similarity between the two systems [Raghu, M. et al, NeurlIPS, (2017)]. In the content of this article, it would be the similarity of the brain and the fitted RNN. Fig 2 shows a successful example in the motor cortex and muscle activity. Please refer to detail in [Sussillo, D. et al, Nat. Neurosci, (2015)]. The most successful example at the local level I would say is the emergence of grid cell while the agent is performing navigation task [Cueva, C. J., &amp; Wei, X. X., ICLR, (2018)]. Despite that $J$ could have poor biological meanings, this doesn’t stop researchers study the complex brain with RNN – a comparably simpler model. Although indeed we know little about RNN itself, replacing the biological brain with a model allows us to do much more virtual experiments. The common one is to linearize RNN around the slow points and fix points, see figure 3 and [Vyas, S. et al, (2020)]. Some ThoughtsIn my perspective, the biggest challenge of using RNN is how to make it more biologically. This requires more theoretical work. How to shift novel analysis tools in Deep learning is another promising research direction. SVCCA could reveal the individual difference of two RNN with different initial conditions [Raghu, M. et al, NeurlIPS, (2017)]. If we can assign each people with one RNN, it is possible to find their individual or group differences. This should shed some light on medical research. We also need to go beyond linearizing RNN, more nonlinear analysis tools such as Riemann curvature, I believe could also provide precious information about the brain.","categories":[{"name":"Comments","slug":"Comments","permalink":"https://ageyy.github.io/neuroblog/categories/Comments/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"https://ageyy.github.io/neuroblog/tags/RNN/"},{"name":"Dynamical System","slug":"Dynamical-System","permalink":"https://ageyy.github.io/neuroblog/tags/Dynamical-System/"}]},{"title":"Noninvasive electromagnetic source imaging of spatiotemporally distributed epileptogenic brain sources","slug":"fastires","date":"2020-10-20T02:30:42.000Z","updated":"2020-10-29T16:00:52.433Z","comments":true,"path":"2020/10/20/fastires/","link":"","permalink":"https://ageyy.github.io/neuroblog/2020/10/20/fastires/","excerpt":"A summary of Abbas Sohrabpour, et al. More detail please refer to the original paper or my powerpoint.","text":"A summary of Abbas Sohrabpour, et al. More detail please refer to the original paper or my powerpoint. SummaryUnderstanding the dynamics of our brain network is not only important for cognitive science, but also for clinical applications. A good spatiotemperal imaging method should occupy at least three characteristics: high spatial resolution, high temporal resolution and wide spatial convergence. Traditional methods unfortrunately cannot satisfy the three all. fMRI has good spatial resolution and extent, yet poor temporal resolution. Invasive method (e.g, iEEG) have the first two properties but can measure only a small region, and may cause infection. EEG/MEG on the other hand has the latter two properties. How to improve the spatial resolution falls one of the central issues in computational neuroscience. EEG electrode is actually measuring the summation of potential from all neurons. The activation of a small population of neurons is often characterized as a dipole. With this simplification, the detected signal (scalp potential) can be calculated by the Maxwell equation [Neuronal Basics of EEG / MEG]. Distributed model discretizes the brain into a lot of voxels, each voxel contains one dipole. Hence we derived the discretized form of the EEG-dipole relation.\\begin{equation}\\phi = A s + n\\end{equation} where $\\phi$ is the EEG signal, $s$ is a vector contains the parameters of all dipoles, and $A$ is a constant matrix. For example, the activation of a particular region of the brain is equivalent to some elements of $s$ are larger than those nearby dipoles. $n$ is the noise. Forward problem, i.e. knowing $s$ to calculate $\\phi$, is easy. Yet its inverse problem, from $\\phi$ to $s$ is ill-posed. Naively we would like to minimize the L2 norm\\begin{equation}s^{est} = argmin_s |\\phi - As|_2.\\label{l2norm}\\end{equation} Yet there are many more parameters than electrodes. Thus we need to add a penalty to the number of parameters. Finally equation (\\ref{l2norm}) can be reformulated as [Sohrabpour, A., et al., (2016)]\\begin{equation}s^{est} = argmin_s |V s|_1 + \\alpha |s|_1\\end{equation} subject to\\begin{equation}(\\phi - As) \\Sigma^{-1} (\\phi - As) &lt; \\beta\\end{equation} where $V$ is the gradient operator which introduce a strong penalty on the number and strength of the edges of the activation region. $\\alpha$ is a hyperparameter but can be evaluated through L-curve [Sohrabpour, A., et al., (2016)]. Minimizing the first term is trying to make the activation of neurons aggregate into limited regions. $\\Sigma$ is the correlation of the noise. And $\\beta$ controls the tolerence of noise level. If the distribution of the noise is known, $\\beta$ can be estimated properly. Overall, this optimization problem can be solved numerically. Now we could let time involved. By using ICA, the scalp potential can be decomposed into spatial and temporal conponents. This temporal basis should consist with the underlying sources, i.e.\\begin{equation}\\boldsymbol{s}(t) = \\Sigma_i \\boldsymbol{s_i} \\otimes \\boldsymbol{a}_i(t)\\end{equation} where we project source $s$ into subregions of the brain, $s_i$, and only look on the overall evolution of the region with time, instead of looking into every voxels. Similar with the above procedure, we could again construct a correspond optimization problem to solve $\\boldsymbol{s}(t)$ from $\\boldsymbol{\\phi}(t)$ numerically. DisscusionFAST-IRES is an effective method for revealing the spatiotemporal information of the brain network. It has the following main features: Only few parameters need to be adjusted manually. This can largely decrease the human bias. Easy to distinguish the source region from the background. The gradient operator add strong penalty on the edge of the active region. Thus only limited number of active regions are allowed. Noninvasive. This is quite important in practice. Since invasive methods (e.g. iEEG) is expensive and could possibly cause infection.","categories":[{"name":"Ref Reading","slug":"Ref-Reading","permalink":"https://ageyy.github.io/neuroblog/categories/Ref-Reading/"}],"tags":[{"name":"EEG/MEG","slug":"EEG-MEG","permalink":"https://ageyy.github.io/neuroblog/tags/EEG-MEG/"},{"name":"Epilepsy","slug":"Epilepsy","permalink":"https://ageyy.github.io/neuroblog/tags/Epilepsy/"},{"name":"Electrophysiological Source Imaging","slug":"Electrophysiological-Source-Imaging","permalink":"https://ageyy.github.io/neuroblog/tags/Electrophysiological-Source-Imaging/"}]},{"title":"Theory of Grid Cell","slug":"gridcell","date":"2020-09-28T04:10:38.000Z","updated":"2020-10-26T06:05:01.178Z","comments":true,"path":"2020/09/28/gridcell/","link":"","permalink":"https://ageyy.github.io/neuroblog/2020/09/28/gridcell/","excerpt":"An brief overview on my talk, Theory of Grid cell. Please refer to my powerpoint for figures and more detail.","text":"An brief overview on my talk, Theory of Grid cell. Please refer to my powerpoint for figures and more detail. Overview of the TalkNavigation is one of the fundamental cognitive ability exist in a wide range of animals. Yet its mechanism remains unclear. There are several types of cells are related to this ability. For example, Place cell has a receptive field to only one particular location (video). Grid cell has fasnating hexagonal receptive field (Nancy’s talk, more biological properties please refer the original paper [Torkel Hafting et al., 2005]). The biological meaning of the place cell is obvious, yet how the hexagonal pattern assist the agent to navigate remains unclear. Mechanistic Model succeeded in reproducing the hexagonal pattern though manually constructing the wiring of the grid cell networks.[Y Burak &amp; I Fiete, 2009][M Fuhs et al, 2006] Yet it didn’t convincely exlain why should the cells has such architechture. Nomative model, on the other hand, have much clearer picture by connecting the grid cell with the place cell or/and head cell etc. 1-layer model used the place cell information as input, few numbers as output, and Hebb rule as learning algorithm, plus a non-negative constrain, to do an unsupervised learning. The weight between input and output layer superisingly has the hexagonal pattern. [Yedidyah Dordek et al.]. This network can be interpreted as an encoder, which encodes the place cell information to few numbers. Therefore grid cell will naturally occur if you want to conpress the place cell information. The second nomative model is based on the RNN[C Cueva &amp; X wei, 2018]. The velocity (speed and direction) of the agent is the input, the output is the place cell information. RNN is used as the hidden layer to convert velocity into location. After traning with sufficient data, the hidden unit apears to have hexagonal receptive field. Furthermore, other type of cells, such as border cell and band cell are also observed. Therefore, grid cell in here is for decoding velocity into location. The above models can be unified with one theory [Ben Sorscher et al., 2019]. The common sub-problem in the two nomative model is how can you related the grid cell to produce the place cell information. Therefore the following loss function is constructed\\begin{equation}L = | P - \\hat{P} |,\\label{loss}\\end{equation} where $P$ is the place cell information, $\\hat{P}$ is the predicted place cell information based on the grid cell. With some simplification, the author analytically showed the grid pattern will occur while minimizing this loss function, so the two nomative models are unified. While the dynamic equation in the mechanistic model is only the gradient descent equation of the Lagrangian of eqaution (\\ref{loss}). Disscusion[Ben Sorscher et al., 2019] gave very good conclusion on the grid cell problem. Here I only mention my one thought. The above theory not only partially solved the grid cell problem, but also showed a very clear logic of how can we understanding our brain. The first step is to construct a proper artificial network which can reproduce at least some properties of the sensory cortex. This should be done by creativly setting the architechture, learning rule and objective function. This re-representation could offer clear explaination of the original biological cortex (for example, see visual cortex in my another post). Then, a theory should be applied in explaining the artificial neural network, which can offer more insight on how our brain works.","categories":[{"name":"Ref Reading","slug":"Ref-Reading","permalink":"https://ageyy.github.io/neuroblog/categories/Ref-Reading/"}],"tags":[{"name":"Grid Cell","slug":"Grid-Cell","permalink":"https://ageyy.github.io/neuroblog/tags/Grid-Cell/"},{"name":"Navigation","slug":"Navigation","permalink":"https://ageyy.github.io/neuroblog/tags/Navigation/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://ageyy.github.io/neuroblog/tags/Machine-Learning/"}]},{"title":"Understanding Sensory Cortex via Hierachical Convolutional Neural Networks.","slug":"hcnn","date":"2020-09-16T13:49:48.000Z","updated":"2020-10-18T17:50:40.081Z","comments":true,"path":"2020/09/16/hcnn/","link":"","permalink":"https://ageyy.github.io/neuroblog/2020/09/16/hcnn/","excerpt":"The skeleton of this short review is mainly based on Using goal-driven deep learning models to understanding sensory cortex","text":"The skeleton of this short review is mainly based on Using goal-driven deep learning models to understanding sensory cortex IntroductionOne ultimate goal of the Neuroscience is to predict the reponse of even a single neuron, given a particular stimulus (i.e. how the neuron encode the information). In terms of this goal, visual cortex is probably the most well-understood cortex. Therefore this short review will start from brief biological introduction to the central visual pathway. Then move to how the traditional methods taken efforts in making predition of the response. However, these traditional method failed to predict the response of neurons in higher order of cortex region, such as in V4 or inferior temporal (IT) cortex. Therefore, in section 3, we introduce a generative model: Hierachical Convolutional Neural Networks (HCNN), equip it with goal-driven philosiphy, to obtain pretty good predition even with higher order of cortex region. The final section includes some potentials and limitations of this HCNN $+$ goal-driven method. Central Visual PathwayCentral visual pathway means how the information of the images (represented by photons) is converted into the electric signal, and be transmitted in the cortex. The photons firstly hit the retina in our eyes, change the conductance of the channels in our photo-receptors, thus change strength the electuric signals. This signals is transmitted by ganglion cells along the optic nerve to reach the Lateral geniculate nucleus (LGN) in the thalamus (FIG. 1). Optic radiation radiate the signals in LGN to the V1 cortex in the most posterior of the occipital lobe. Now there are basicaly two pathway for the signal to travel. (FIG.) [S. Williams &amp; L. White, 2013] Dorsal pathway: V1, V2, V3, V3A and MT etc. This pathway is thought of responsible for the “where” problem: what is the positional relation of the objects in the image, in which direction does the object move… Ventral pathway: V1, V2, V4 and IT etc. This pathway is thought of responsible for the “what” problem, such as object recogization. One goal of today’s neuroscience is to understanding how the neurons in the above regions response to different stimulus (i.e. images). More formally, the stimulus is a matrix $S$, with each entry represents the photon density, the questions finding the firing rate of the neurons in a very small regions (so they can be thought of react to the stimulus homogenusly) $r$ as a function of matrix $S$. Traditional MethodGabor wavelet filterThe most straight forward model of the above problem is receptive field\\begin{equation}r = \\sum_{i,j} R_{i,j}S_{i,j}\\end{equation} where $R$ is the receptive field. It can be either a matrix (descret positions) or function R(x, y) (continum positions). According to some observation, JP Jones etal in year 1987 propsed that R is actually a gabor wavelet [J Jones &amp; L Palmer]\\begin{equation}R(x, y) = K \\text{exp}(-1/2(x_g^2 / a^2 + y_g^2 / b^2)) \\text{cos}(-2\\pi(U_0x + V_0y) - P),\\label{gaborwave}\\end{equation} where $x_g$ and $y_g$ are the rotation transfomation of the original coordinate space\\begin{equation}x_g = x \\text{cos}(\\theta) - y \\text{sin}(\\theta);y_g = x \\text{sin}(\\theta) + y \\text{cos}(\\theta)\\end{equation} The rest of parameters in equation (\\ref{gaborwave}) are constant. The fitting result shows pretty good result of the neurons in the V1 cortex (FIG.3) However, this method only tells what the receptive field should look like, but didn’t clearly expain why they should be gabor wavelet. Learning the Neural Response via Efficient CodingOn the contrary, efficient coding could give a clearer biological meaning. An interpretation of the function of neurons is that, different neurons are developed to capture different aspects of the images. For example, neuron 1 will only detect the image information in the horizonal direction, neuron 2 will only look at into the infomation in another direction. So neurons could actually tears the original image into some simple parts which is easier for further analysing. Actually this idea could be achieved by gabor function: the response via different gabor wavelet filters can indeed capture different aspects of the original image. Yet efficient coding is a more powerful method because it could learn the response automatically. With the above idea, the image can be represented by a set of basis [BOlshausen &amp; D. Field, 1996]\\begin{equation}S(x, y) = \\sum_{i} a_i \\phi_i(x, y).\\end{equation} where index $i$ represents neuron $i$. Our brain must evolved at some point between the ability (how well we can understand the image) and economy (it should take as fewer neurons as possible). This two aspects hint us to write down the following loss function\\begin{equation}L = - [\\text{perserve information}] - \\lambda [\\text{sparseness of $a_i$}]\\label{lossinfo}\\end{equation} where $\\lambda$ controls the importance of economy. Perserve information could be the L2 error\\begin{equation}[\\text{perserve information}] = - [S(x, y) - \\sum_{i} a_i \\phi_i(x, y)]^2\\end{equation} and\\begin{equation}[\\text{sparseness of $a_i$}] = - \\sum_i S(\\frac{a_i}{\\sigma})\\end{equation} Equation (\\ref{lossinfo}) can be minimized though gradient descent. We could use nature scence as the traning set, and do the unsupervised learning so could obtain the basis (FIG. 4). It correctly shows how different neurons capture different aspects of the original image. Fitting Networks to Neural DataThis method is even more straightforward. We could obtain the stimulus and the response of the neurons from the experiment, therefore we could link this two quantities directlly though some statistics method. This method apears to be succesful in visual area V1, auditory area A1 etc. [R deCharms and A Zador, 2000] Difficulties with Deeper NetworksThe above models are successful in the primary cortex (V1). However, the higher order region like V4 apears to be too complex so that either asumptions in the above method become invalid nor too difficult to fit due to the too many parameters. Therefore new method is needed. HCNN and Goal-driven MethodAlthough we don’t exactly know how neurons in the cortex wiring together, we could roughly divide cortex into layers (under the meaning of connections, not anatomy relation). For example, V1, V2 are different layers. There are infomation flows between layers to layers. One good anology of the cotex is Hierachical Convolutional Neural Networks (FIG. 5). Each entry in one layers is a number, corresponds to the firing rate of one small region of neurons. Layers are connected though linear-non-linear (LN) operation: filter, threshold, pool and normalize. Each sub-operation can roughly corresponds to some features of the cortex. For example, threshold is the activation function of one neuron under a stimulus. HCNN contains majorly two kinds of parameters: 1. discrete paramters, includ the number of layers, shape of the filters etc. 2. continuous paramters, the weights on the filter and fully connected layers. From the traditional method, we know it is difficult to findout the stimulus-response relation. Therefore in goal-driven method, we temperally don’t consider this problem. Instead, we consider how to use HCNN to reproduce the cognitive ability, for example, in ventral visual patheway, how to recognize objects. The parameters above are configured by optimizing the performance of the cognitive abilitt. Surperising, if we detect the response of the neurons in hidden layer, it will give good prediction of how the real neuron response (FIG. 5). The most powerful aspect of HCNN I think is that, it can unify different sensory cortex into one architechture. HCNN is not only applicable to visual cortex, but with some modification, also applicable to auditory cortex. This implies that our brain was actually a giant, simple HCNN. The evolution force us to “learn” the exact parameters to achieve different congnative ability. Actually, if part of funtion in our brain is injured, we could possible use the other part of the brain to perform the same task, which means the cortex specialied in one function can smoothly transformed into another specialization. [S. Carron ect.] DisscusionMost of the potential and limitations are discussed in [D Yamins ectal., 2018]. Here I would only add some points. The most limitation of the HCNN I suppose is the architechture. Convolution apears only for some invariant properties in the image. It is likely that “filter” neurons developed same parameters, so that we could use single filter to replace them. However, there’s no reason that the other sensory cotex have some similar symmetry. Further, recurrent seems exist everywhere in the cortex, yet this important feature is not inclued in the HCNN model. Therefore better understanding on how to coorperate RNN in the brain to the machine learning algorithm is needed. [G Yang &amp; X Wang, 2020] Further, even we could reproduce the response of neuron via HCNN model, we still don’t know why they response like that. HCNN model, unfortrunately lack of good interpretability. This is related to the long existing reverse engineering problem. Image Ref[1] S. Williams &amp; L. White, 2013[2] J Jones &amp; L Palmer[3] BOlshausen &amp; D. Field, 1996[4] BOlshausen &amp; D. Field, 1996","categories":[{"name":"Ref Reading","slug":"Ref-Reading","permalink":"https://ageyy.github.io/neuroblog/categories/Ref-Reading/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://ageyy.github.io/neuroblog/tags/Machine-Learning/"},{"name":"Visual System","slug":"Visual-System","permalink":"https://ageyy.github.io/neuroblog/tags/Visual-System/"},{"name":"HCNN","slug":"HCNN","permalink":"https://ageyy.github.io/neuroblog/tags/HCNN/"}]},{"title":"Modeling of Simple Neural Network","slug":"biophysics","date":"2020-09-09T09:15:48.000Z","updated":"2020-10-18T17:50:48.745Z","comments":true,"path":"2020/09/09/biophysics/","link":"","permalink":"https://ageyy.github.io/neuroblog/2020/09/09/biophysics/","excerpt":"Mathematical modeling from sub-cell components of a neuron to simple neural network. As part of course Introduction to Neural Computation","text":"Mathematical modeling from sub-cell components of a neuron to simple neural network. As part of course Introduction to Neural Computation IntroductionResearchers study our neural system in different levels: from components of a single cell, such as membrane, to the connection of two cells (synapsis), and further to the complex network of the whole brain. This summary trying to find a straight pathway of the modeling from sub-cellcular level to simple neural network. Please refer Introduction to Neural Computation for more detail. MembraneThe membrane has roughly two parts that are vital in the neural activity: phospholipid bilayer and channels (FIG. 1). Phospholipid bilayer is permeable to water, oil etc., yet impereable for ions. This attribution is similar to semi-permeable membrane. Ions carry charges, if the charge densities of the two sides of phospholipid bilayer (intra cellular space and extra-cellular space) are different, there will be a voltage difference across cell membrane. In all, because of the existence of phospholipid bilayer, the cell has a function similar to capacitor\\begin{equation}c V = Q\\label{ohm}\\end{equation} where $c$ is the capacity per unit area, $Q$ is the charge density differences and $V$ is the voltage accross the cell. The voltage of the extra-cellcular space is set to be zero, therefore $V$ here is also the voltage of intra-cellular space, which is roughly $-70 \\text{mV}$ to $ -50 \\text{mV}$ as measured by the experiment. There are two types of channels: ion channels and metabolic channel. The effect of metabolic channels are more indirect and complex, which is not the central concern of this summary. Ion channel behaviors as the gate of the membrane. It allows only certain types of ions to pass through the membrane, thus produce inward or outward current flow. For example, Na-gated channel is only permeable to Na$^+$, when the channel is open . One good approximation of the channel is resistor, govern by the Ohm’s law\\begin{equation}I = G_i (V - V_i) \\label{olm}\\end{equation} where $G_i$ is the conductance which can depend on the votlage and time, and $V_i$ is the equilibrium voltage (constant for a certain type of channel) results from the balance of electric force and diffusion (FIG. 2). Index $i$ here means $i$th type of ion channel. Positive $I$ means outward current. Single CellDifferentiate on both side of equation (\\ref{ohm}) and use equation (\\ref{olm}), we have an equation for single cell\\begin{equation}c \\frac{dV}{dt} = - G_L (V - V_L) - I_e\\label{ifmodel}\\end{equation} where $I_e$ is the extra-induced current, such as the experimentist inject ions to the cell (negtive $I_e$). Index $L$ is leakage channel, which is a mathematically equivalent channel of a partial function of some types of channel (largely fall into K-selective channel). The rest of effects of channels can be simplified as the following statement: If $V &gt; V_{th}$, $V = 50 \\text{mV}$, then quickly fall to $V_{res}$. This effect is often introduced by Na-selective channels. Typical value of $V_{res}$ is $-65 mV$. There are some choices for $V_{th}$, but values like $-30 mV$ is sufficient to fit a good result. When this statement is triggered, we say the neuron fired, or it made a spike. This single-cell model also be named Integrate and Firing Model. This model contains one resistor (leakage channel), one conductance (membrane), one external current source ($I_e$), and voltage source (FIG. 3). There are lots more sophisticated models, such as HH model, which replace the statement with more realistic channels as additional terms in equation (\\ref{ifmodel}). Equation (\\ref{ifmodel}) can be collected with a more compact form\\begin{equation}\\tau \\frac{dV}{dt} = -V + V_{\\infty}, \\label{ifmodelS} \\end{equation} where $\\tau = c / G_L$ and $V_{\\infty} = (V_L - I_e) / G_L$. If $G_L$ here is a constant, solution of equation [ifmodelS] is\\begin{equation}V = (V_L - V_{\\infty}) e^{-t/\\tau} + V_{\\infty}.\\end{equation} The firing pattern is shown in FIG. 4 Information is stored in the firing rate of neurons instead of the exact value of voltage. Firing rate is defined as the number of spikes in a unit time. The time spacing between two spikes can be calculated from equation (\\ref{ifmodelS}) by calculate the time that $V$ grows from $V_{res}$ to $V_{th}$. The inverse of time spacing is firing rate, as shown in FIG. 5. In all, we could conclude the relation with\\begin{equation}v = F[I_e]\\label{fireateeq}\\end{equation} where $v$ is the firing rate, and $F$ is some function has the shape in FIG. 5 SynapseBefore moving to neural network, we need to understand how two neurons connect each other by synapse. FIG. [syn] shows the biological structure of the synapse. When the pre-neuron is fired, the depolyzation will open the Ca-channel, induces an inward flow of Ca$^{2+}$. This Ca$^{2+}$ will release the transmitters in the presynapse. Transmitter in the synapse cleft then attach to particular receptors in the post-synapse, open ion-channel to allow Na$^+$ flow into the post-synapse. This final lead to the depolarization of the post-neuron. The current flow into the post-neuron can be described by\\begin{equation}I_{syn} = - G_{syn}(t) (V - V_{syn}), \\label{isyn}\\end{equation} where $V_{syn}$ is about $0$ mV, which is the reversal potential of the post-synapse. Conductance $G_{syn}(t)$ behaviors like a exponential decaying function. If there’s no transmitters in the synapse cleft, ion channels in the post-synapse are closed, $G_{syn}(t) = 0$ mV. A spike of the presynapse will suddenly release a large amount of transmitters, therefore largely increase $G_{syn}(t)$. Then the transmitters will graduatly either diffuse away or be decomposed by glials, which lead to the slow decay of the conductance. The conductance induced by single spike is often approximated by the exponential function\\begin{equation}K(t) = G_{max} e^{-t / \\tau}.\\end{equation} The whole conductance under a train of spikes is\\begin{equation}G(t) = \\sum_iK(t) \\delta_{t,t_i}\\label{spikeraw}\\end{equation} where there is a spike at time $t_i$. Equation (\\ref{spikeraw}) can be simplified with the spike train $S(t) = \\sum_i\\delta(t - t_i)$\\begin{equation}G(t) = \\int{d\\tau}K(\\tau)S(t - \\tau)\\end{equation} Simple Neural NetworkConsider a network with only two neurons attached. The working status of each neuron is solely stated by its firing rate. So we need to findout the firing-rate relation between neurons. For the single neuron, the only source of external current is from its pre-neuron, i.e. $v = F[I_{syn}]$ from equation (\\ref{fireateeq}). Yet $I_{syn}$ followed by equation (\\ref{isyn}) is still too complex to be pratical. Hence we further throw away the voltage term, and treat $G_{syn}(t)$ as voltage-independent,\\begin{equation}I_{syn} = \\int{d\\tau}K(\\tau)S(t - \\tau).\\end{equation} we could extract a factor $w$ so that the rest of $U(\\tau) = K(\\tau) / w$ can be normalized,\\begin{equation}\\int{d\\tau}U(\\tau) = 1\\end{equation} The convolution of a narrow (correnspond to short time of non-zero $K(\\tau)$) window $U(\\tau)$ with spike train $S(\\tau)$ is exactly the firing rate of the presynapse $u$. Overall, we have\\begin{equation}v = F[w u].\\end{equation} This equation reveals the simple relation of the network. Larger $w$ means the two neurons are more strongly connect, only a slow firing rate of the pre-neuron is enough for triggering the post-neuron. This biologically could mean there are more channels in the post-synapse. Different presynapse will attarch to different sites of the postsynapse. Furthermore, the post-neual layer can also attarch to each other as shown in FIG. 7. The overall effect is the summation of all their current.\\begin{equation}\\boldsymbol{v} = F[W\\boldsymbol{u} + M \\boldsymbol{v}],\\end{equation} where $W, M$ are matrices, arrays are written as bold letters. This model is called rate model, which is the fundation of the lots of deep learning architechture. ConclusionThe membrane of the neuron is a combination of resistor and capacity. Different conductance of different ion channels allow the neuron to fire, and soon go back to the resting potential. A spike from the pre-neuron will trigger a bit depolarization of the post-neuron. Once the potential is high enough, the post-neuron will also make spikes. Despite the complicated procedure for generating a spike, the exact voltage is unimportant. The real information is stored in the frequency of the spikes, which can be described using a single number – firing rate. By carefully simplifing the synapse, one could obtain the relation between the firing rates of different neurons, thus have an equation for the neuron network. This neuron network lies in the basis current deep network algorithms. Ref[1] Wiki: cellular membrane[2] Chapter 3 of Neuroscience: Exploring the Brain[3] Lecture 3 of MIT OpenCourseWare: Introduction to Neural Computation[4] Chapter 5 of Neuroscience: Exploring the Brain[5] Lecture 18 of MIT OpenCourseWare: Introduction to Neural Computation","categories":[{"name":"Ref Reading","slug":"Ref-Reading","permalink":"https://ageyy.github.io/neuroblog/categories/Ref-Reading/"}],"tags":[]}],"categories":[{"name":"Ref Reading","slug":"Ref-Reading","permalink":"https://ageyy.github.io/neuroblog/categories/Ref-Reading/"},{"name":"Ideas","slug":"Ideas","permalink":"https://ageyy.github.io/neuroblog/categories/Ideas/"},{"name":"Comments","slug":"Comments","permalink":"https://ageyy.github.io/neuroblog/categories/Comments/"}],"tags":[{"name":"RNN","slug":"RNN","permalink":"https://ageyy.github.io/neuroblog/tags/RNN/"},{"name":"Dynamical System","slug":"Dynamical-System","permalink":"https://ageyy.github.io/neuroblog/tags/Dynamical-System/"},{"name":"Working Memory","slug":"Working-Memory","permalink":"https://ageyy.github.io/neuroblog/tags/Working-Memory/"},{"name":"Autistic Spectrum Disorder","slug":"Autistic-Spectrum-Disorder","permalink":"https://ageyy.github.io/neuroblog/tags/Autistic-Spectrum-Disorder/"},{"name":"EEG/MEG","slug":"EEG-MEG","permalink":"https://ageyy.github.io/neuroblog/tags/EEG-MEG/"},{"name":"Epilepsy","slug":"Epilepsy","permalink":"https://ageyy.github.io/neuroblog/tags/Epilepsy/"},{"name":"Electrophysiological Source Imaging","slug":"Electrophysiological-Source-Imaging","permalink":"https://ageyy.github.io/neuroblog/tags/Electrophysiological-Source-Imaging/"},{"name":"Grid Cell","slug":"Grid-Cell","permalink":"https://ageyy.github.io/neuroblog/tags/Grid-Cell/"},{"name":"Navigation","slug":"Navigation","permalink":"https://ageyy.github.io/neuroblog/tags/Navigation/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://ageyy.github.io/neuroblog/tags/Machine-Learning/"},{"name":"Visual System","slug":"Visual-System","permalink":"https://ageyy.github.io/neuroblog/tags/Visual-System/"},{"name":"HCNN","slug":"HCNN","permalink":"https://ageyy.github.io/neuroblog/tags/HCNN/"}]}